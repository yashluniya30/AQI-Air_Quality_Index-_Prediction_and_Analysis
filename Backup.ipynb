{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 1 - PRELIMINARY AND EXPLORATORY ANALYSIS OF STATION_CSV AND STATION_DAY CSV\n",
    "\n",
    "# Purpose\n",
    "<b>Finding meaningfull insights about AQI from diffrent stations in india. \n",
    "\n",
    "https://www.diva-portal.org/smash/get/diva2:1681590/FULLTEXT02 in this reasearch paper the creators have only dumped the data into their machine learning models but did not explore the data itself. </b>\n",
    "\n",
    "# what is AQI?\n",
    "\n",
    "<b> The Air Quality Index (AQI) is used for reporting daily air quality. It tells you how clean or polluted your air is, and what associated health effects might be a concern for you. The AQI focuses on health effects you may experience within a few hours or days after breathing polluted air. </b>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import mysql.connector\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "import seaborn as sns\n",
    "import geopandas as gpd\n",
    "import folium\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ________________________________________________________________________________\n",
    "# 2. showing columns and heads of station and station_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish connection to MySQL database\n",
    "conn = mysql.connector.connect(\n",
    "    host=\"localhost\",\n",
    "    user=\"root\",\n",
    "    password=\"3263\",\n",
    "    database=\"air_quality_db\",\n",
    "    allow_local_infile=True\n",
    ")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Creating database\n",
    "cursor.execute(\"CREATE DATABASE IF NOT EXISTS air_quality_db;\")\n",
    "cursor.execute(\"USE air_quality_db;\")\n",
    "\n",
    "# Drop tables if they exist\n",
    "cursor.execute(\"DROP TABLE IF EXISTS station_day;\")\n",
    "cursor.execute(\"DROP TABLE IF EXISTS stations;\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"\"\"\n",
    "    CREATE TABLE stations (\n",
    "        StationId VARCHAR(20) PRIMARY KEY,\n",
    "        StationName VARCHAR(255),\n",
    "        City VARCHAR(100),\n",
    "        State VARCHAR(100),\n",
    "        Status VARCHAR(20)\n",
    "    );\n",
    "\"\"\")\n",
    "\n",
    "# Create the 'station_day' table\n",
    "cursor.execute(\"\"\"\n",
    "    CREATE TABLE station_day (\n",
    "        StationId VARCHAR(10),\n",
    "        Date DATE,\n",
    "        PM2_5 FLOAT NULL,\n",
    "        PM10 FLOAT NULL,\n",
    "        NO FLOAT NULL,\n",
    "        NO2 FLOAT NULL,\n",
    "        NOx FLOAT NULL,\n",
    "        NH3 FLOAT NULL,\n",
    "        CO FLOAT NULL,\n",
    "        SO2 FLOAT NULL,\n",
    "        O3 FLOAT NULL,\n",
    "        Benzene FLOAT NULL,\n",
    "        Toluene FLOAT NULL,\n",
    "        Xylene FLOAT NULL,\n",
    "        AQI FLOAT NULL,\n",
    "        AQI_Bucket VARCHAR(50),\n",
    "        FOREIGN KEY (StationId) REFERENCES stations(StationId)\n",
    "    );\n",
    "\"\"\")\n",
    "\n",
    "print(\"Tables created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable LOCAL INFILE for MySQL\n",
    "cursor.execute(\"SET GLOBAL local_infile = 1;\")\n",
    "\n",
    "# Load data into 'stations' table (Handle NULL values)\n",
    "cursor.execute(\"\"\"\n",
    "    LOAD DATA LOCAL INFILE '/Users/pulkitsoni/Air_Quality_Analysis_and_Prediction_for_Indian_Cities_and_States/stations.csv'\n",
    "    INTO TABLE stations\n",
    "    FIELDS TERMINATED BY ',' \n",
    "    ENCLOSED BY '\"' \n",
    "    LINES TERMINATED BY '\\n'\n",
    "    IGNORE 1 ROWS\n",
    "    (StationId, StationName, City, State, Status)\n",
    "    SET StationId = NULLIF(StationId, ''),\n",
    "        StationName = NULLIF(StationName, ''),\n",
    "        City = NULLIF(City, ''),\n",
    "        State = NULLIF(State, ''),\n",
    "        Status = NULLIF(Status, '');\n",
    "\"\"\")\n",
    "\n",
    "# Load data into 'station_day' table (Handle NULL values)\n",
    "cursor.execute(\"\"\"\n",
    "    LOAD DATA LOCAL INFILE '/Users/pulkitsoni/Air_Quality_Analysis_and_Prediction_for_Indian_Cities_and_States/station_day.csv'\n",
    "    INTO TABLE station_day\n",
    "    FIELDS TERMINATED BY ',' \n",
    "    ENCLOSED BY '\"' \n",
    "    LINES TERMINATED BY '\\n'\n",
    "    IGNORE 1 ROWS\n",
    "    (StationId, Date, PM2_5, PM10, NO, NO2, NOx, NH3, CO, SO2, O3, Benzene, Toluene, Xylene, AQI, AQI_Bucket)\n",
    "    SET PM2_5 = NULLIF(PM2_5, ''),\n",
    "        PM10 = NULLIF(PM10, ''),\n",
    "        NO = NULLIF(NO, ''),\n",
    "        NO2 = NULLIF(NO2, ''),\n",
    "        NOx = NULLIF(NOx, ''),\n",
    "        NH3 = NULLIF(NH3, ''),\n",
    "        CO = NULLIF(CO, ''),\n",
    "        SO2 = NULLIF(SO2, ''),\n",
    "        O3 = NULLIF(O3, ''),\n",
    "        Benzene = NULLIF(Benzene, ''),\n",
    "        Toluene = NULLIF(Toluene, ''),\n",
    "        Xylene = NULLIF(Xylene, ''),\n",
    "        AQI = NULLIF(AQI, ''),\n",
    "        AQI_Bucket = NULLIF(AQI_Bucket, '');\n",
    "\"\"\")\n",
    "\n",
    "print(\"Data loaded successfully with NULL values!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute SQL query to fetch data\n",
    "cursor.execute(\"SELECT * FROM stations\")\n",
    "rows = cursor.fetchall()\n",
    "\n",
    "columns = [desc[0] for desc in cursor.description]\n",
    "station_df = pd.DataFrame(rows, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute SQL query to fetch data\n",
    "cursor.execute(\"SELECT * FROM station_day\")\n",
    "rows = cursor.fetchall()\n",
    "\n",
    "columns = [desc[0] for desc in cursor.description]\n",
    "station_day_df = pd.DataFrame(rows, columns=columns)\n",
    "\n",
    "conn.commit()\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If can't connect to the database we can directly use these csv files\n",
    "\n",
    "# station_df = pd.read_csv(\"stations.csv\")\n",
    "# station_day_df = pd.read_csv(\"station_day.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_day_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_day_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_day_df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# ________________________________________________________________________________\n",
    "# 3. making the station merged csv (main csv we will work with)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining the datasets as it makes analysing the data easier\n",
    "\n",
    "# we dont need the name of the station and the satus if it is active or not so dropping these columns\n",
    "\n",
    "station_df = station_df.drop(columns=[\"StationName\",\"Status\"])\n",
    "\n",
    "station_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging dataframes based on StationId column\n",
    "\n",
    "station_merged_df = pd.merge(station_df, station_day_df, on=\"StationId\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# ________________________________________________________________________________\n",
    "# 4. preliminary data analysis of station_merged_df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. general shape\n",
    "station_merged_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. some values\n",
    "station_merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. columns\n",
    "\n",
    "station_merged_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 data distribution\n",
    "\n",
    "station_merged_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AQL Category, Pollutants and Health Breakpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![aqi thresholds](https://user-images.githubusercontent.com/91218998/226107225-4e78b162-7844-43d9-88c4-17caf21f76da.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From the above table we see that over the years from 2015 to 2020 the air has been moderately polluted "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_merged_df.isnull().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* There are 21010 rows in the dataset where AQI, and AQI_Bucket are missing\n",
    "* These rows are useless as they dont have our target value so we will remove them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will only selecting values where isnull is not true (meaning the value is not null)\n",
    "\n",
    "mask = station_merged_df[\"AQI\"].isnull() == False\n",
    "station_merged_df = station_merged_df[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_merged_df.isnull().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The missing values now are numerical columns \n",
    "* The best way fill those values would be to replace these values with the mean of the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_merged_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_merged_df.describe().loc[\"mean\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_merged_df = station_merged_df.replace({\n",
    "\n",
    "\"PM2_5\" : {np.nan:80.387649},\n",
    "\"PM10\" :{np.nan:158.557614},\n",
    "\"NO\": {np.nan:23.244401},\n",
    "\"NO2\": {np.nan:35.118825},\n",
    "\"NOx\": {np.nan:43.815139},\n",
    "\"NH3\": {np.nan:28.654163},\n",
    "\"CO\":  {np.nan:1.645650},\n",
    "\"SO2\": {np.nan:12.212651},\n",
    "\"O3\": {np.nan:38.320547},\n",
    "\"Benzene\":  {np.nan:4.011256},\n",
    "\"Toluene\": {np.nan:18.244392},\n",
    "\"Xylene\":  {np.nan:3.368674}})\n",
    "station_merged_df.isnull().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The lines below change date column into datetime pandas datatype and creates 3 columns (year, month, day) then drops the date column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_merged_df[\"Date\"] = pd.to_datetime(station_merged_df[\"Date\"])\n",
    "station_merged_df[\"year\"] = station_merged_df[\"Date\"].dt.year\n",
    "station_merged_df[\"month\"] = station_merged_df[\"Date\"].dt.month\n",
    "station_merged_df[\"day\"] = station_merged_df[\"Date\"].dt.day\n",
    "station_merged_df.drop('Date',axis=1,inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> This is what our station_merged_df looks like after preliminary data analysis </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_merged_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_merged_df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### in station merged df\n",
    "\n",
    "| #  | Column     | Non-Null Count  | Dtype   |    meaning   |\n",
    "|:-: | :------:     | :--------------:  | :-----:   | :---:  |\n",
    "| 0  | StationId  | 108035 non-null | object  |    id of the station   |\n",
    "| 1  | City       | 108035 non-null | object  |    city in which the station is located   |\n",
    "| 2  | State      | 108035 non-null | object  |    state in which the station is located   |\n",
    "| _  | Date       | 108035 non-null | object  |    date of the readings (now removed)   |\n",
    "| 3  | PM2.5      | 86410 non-null  | float64 |    amount of said checmical in the air   |\n",
    "| 4  | PM10       | 65329 non-null  | float64 |    amount of said checmical in the air   |\n",
    "| 5  | NO         | 90929 non-null  | float64 |    amount of said checmical in the air   |\n",
    "| 6  | NO2        | 91488 non-null  | float64 |    amount of said checmical in the air   |\n",
    "| 7  | NOx        | 92535 non-null  | float64 |    amount of said checmical in the air   |\n",
    "| 8  | NH3        | 59930 non-null  | float64 |    amount of said checmical in the air   |\n",
    "| 9  | CO         | 95037 non-null  | float64 |    amount of said checmical in the air   |\n",
    "| 10 | SO2        | 82831 non-null  | float64 |    amount of said checmical in the air   |\n",
    "| 11 | O3         | 82467 non-null  | float64 |    amount of said checmical in the air   |\n",
    "| 12 | Benzene    | 76580 non-null  | float64 |    amount of said checmical in the air   |\n",
    "| 13 | Toluene    | 69333 non-null  | float64 |    amount of said checmical in the air   |\n",
    "| 14 | Xylene     | 22898 non-null  | float64 |    amount of said checmical in the air   |\n",
    "| 15 | AQI        | 87025 non-null  | float64 |    the air quality that day   |\n",
    "| 16 | AQI_Bucket | 87025 non-null  | object  |    AQI catagory ranges from good to severe   |\n",
    "| 17 | year       | 87025 non-null  | float64 |    year of the reading   |\n",
    "| 18 | month      | 87025 non-null  | float64 |    month of the reading   |\n",
    "| 18 | date       | 87025 non-null  | float64 |    day of the reading   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing the csv back\n",
    "station_merged_df.to_csv(\"/Users/pulkitsoni/Air_Quality_Analysis_and_Prediction_for_Indian_Cities_and_States/station_merged.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### things changed in station_merged_csv\n",
    "\n",
    "* dropped all columns where aqi reading was none\n",
    "* numerical missing values were replaced with means of the respective columns \n",
    "* dropped the date column and added day, month, year columns instead"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### this is it for the preliminary analysis of station_merged_df now the fun part"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ________________________________________________________________________________\n",
    "\n",
    "# 5. exploratory data analysis of station_merged_df (the main reason why i made this notebook)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First lets see aqi trends from 2015-2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------------------------------------------------\n",
    "# 1. trend of aqi throughout the years \n",
    "\n",
    "plt.figure(figsize=(17,6), dpi=300)\n",
    "\n",
    "font = {'family' : 'DejaVu Sans',\n",
    "        'weight' : 'bold',\n",
    "        'size'   : 20}\n",
    "\n",
    "mpl.rc('font', **font)\n",
    "\n",
    "mpl.style.use(\"Solarize_Light2\")\n",
    "yearly_grouped = station_merged_df.groupby(\"year\")[\"AQI\"].mean().plot(kind=\"line\",c=\"green\", marker=\"o\",title=\"trend of AQI troughout the years\", ylabel=\"AQI Reading\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* the AQI has decreased steadily over the years esepecially from 2018\n",
    "* the reason 2020 has such a low aqi is due to covid 19 lockdowns  \n",
    "* you can read this news article https://blogs.worldbank.org/endpovertyinsouthasia/india-air-quality-has-been-improving-despite-covid-19-lockdown\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------------------------------------------------\n",
    "# 2. aqi trend of states in india\n",
    "\n",
    "states_group = pd.DataFrame(station_merged_df.groupby(\"State\")[\"AQI\"].mean()).reset_index()\n",
    "states_group = states_group.sort_values(by=\"AQI\", ascending=False)\n",
    "plt.figure(figsize=(20,7), dpi=300)\n",
    "sns.set(font_scale=1.5)\n",
    "sns.barplot(x='State', y='AQI', data=states_group).set(title ='AQI of sates')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Gujrat has the most toxic air with aqi catagory of \"Severe\" even surpassing delhi\n",
    "* Mizoram has the freshest air with an aqi of under 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------------------------------------------------\n",
    "# 3. aqi trend of cities in india\n",
    "\n",
    "cities_group = pd.DataFrame(station_merged_df.groupby(\"City\")[\"AQI\"].mean()).reset_index()\n",
    "cities_group = cities_group.sort_values(by=\"AQI\", ascending=True)\n",
    "mpl.style.use(\"Solarize_Light2\")\n",
    "plt.figure(figsize=(20,7), dpi=300)\n",
    "sns.set(font_scale=2)\n",
    "sns.barplot(x=\"City\", y=\"AQI\", data=cities_group).set(title=\"AQI of citites\", xlabel=\"\", ylabel=\"AQI\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* the aqi of Aizawl is the best (0-100) healthy\n",
    "* the the other hand Ahemdabad is worst with the avrage aqi reaching (400+) severe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------------------------------------------------------------------------------------------------\n",
    "# 4. Yearly/ monthly state_Aqi_from_2015_to_2020 function\n",
    "\n",
    "def state_Aqi_from_2015_to_2020(required_state, year_or_month): # 1, 2\n",
    "\n",
    "    '''\n",
    "        This function shows the aqi trend of a state from 2016 to 2020 on a yearly or monthly basis\n",
    "\n",
    "        steps:\n",
    "        1. checks if year_or_month is 0 or 1 on these basis it shows the monthly aqi trend or yearly aqi trend\n",
    "        2. gets the required state from the user\n",
    "        3. makes mask and then makes required_state_df\n",
    "        4. if 0 years_group is the mean() of all aqi readings from 2016 to 2020 else its all months from 2016 to 2020\n",
    "        5. makes the bar/line plot\n",
    "    '''\n",
    "\n",
    "    if year_or_month == 0: # yearly aqi report\n",
    "\n",
    "        required_state_mask = station_merged_df[\"State\"] == required_state # 3\n",
    "        required_state_df = station_merged_df[required_state_mask] # 3\n",
    "        years_group = pd.DataFrame(required_state_df.groupby(\"year\")[\"AQI\"].mean()).reset_index() # 4\n",
    "        mpl.style.use(\"Solarize_Light2\")  # 5\n",
    "        plt.figure(figsize=(20,5), dpi=300) # 5\n",
    "        sns.set(font_scale=2) # 5\n",
    "        sns.barplot(x=\"year\", y=\"AQI\", data=years_group).set(title=f\"yearly AQI of {required_state}\", xlabel=\"\", ylabel=\"AQI\") # 5\n",
    "\n",
    "    elif year_or_month == 1: # monthly aqi report\n",
    "\n",
    "        required_state_mask = station_merged_df[\"State\"] == required_state # 3\n",
    "        required_state_df = station_merged_df[required_state_mask] # 3\n",
    "        months_group = pd.DataFrame(required_state_df.groupby(\"month\")[\"AQI\"].mean()).reset_index() # 4\n",
    "        mpl.style.use(\"Solarize_Light2\") # 5\n",
    "        plt.figure(figsize=(20,5), dpi=300) # 5\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.plot([\"JAN\", \"FEB\", \"MAR\", \"APR\", \"MAY\", \"JUN\", \"JUL\", \"AUG\", \"SEPT\", \"OCT\",\"NOV\", \"DEC\"], months_group[\"AQI\"], marker=\"o\", c=\"purple\") # 5\n",
    "        plt.title(f\"aqi trend of {required_state} months from 2016-2020\")\n",
    "        plt.ylabel(\"AQI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_list = station_merged_df.State.unique()\n",
    "print(\"The number of states avaible are: \", len(state_list))\n",
    "print()\n",
    "print(state_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "required_state = \"Andhra Pradesh\"\n",
    "year_or_month = 1\n",
    "state_Aqi_from_2015_to_2020(required_state, year_or_month)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* the highest aqi of all months is in jan as BHOGI festival is celebrated in andra pradesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "required_state = \"Rajasthan\"\n",
    "year_or_month = 1\n",
    "state_Aqi_from_2015_to_2020(required_state, year_or_month)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* the highest aqi of all months is in Nov as Diwali festival is celebrated in Rajasthan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "required_state = \"Andhra Pradesh\"\n",
    "year_or_month = 0 # 0 for yeqrly report 1 for monthly\n",
    "state_Aqi_from_2015_to_2020(required_state, year_or_month)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* highest aqi in 2017 with 2020 being the least toxic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------------------------------------------------\n",
    "# 5. Yearly/ monthly city_Aqi_from_2015_to_2020 function\n",
    "\n",
    "# Chennai\n",
    "\n",
    "def city_Aqi_from_2015_to_2020(required_city, year_or_month): # 1, 2\n",
    "\n",
    "    '''\n",
    "        This function shows the aqi trend of a city from 2016 to 2020 on a yearly or monthly basis\n",
    "\n",
    "        steps:\n",
    "        1. checks if year_or_month is 0 or 1 on these basis it shows the monthly aqi trend or yearly aqi trend\n",
    "        2. gets the required city from the user\n",
    "        3. makes mask and then makes required_city_df\n",
    "        4. if 0 years_group is the mean() of all aqi readings from 2016 to 2020 else its all months from 2016 to 2020\n",
    "        5. makes the bar/line plot\n",
    "    '''\n",
    "\n",
    "    if year_or_month == 0: # yearly aqi report\n",
    "\n",
    "        required_city_mask = station_merged_df[\"City\"] == required_city # 3\n",
    "        required_city_df = station_merged_df[required_city_mask] # 3\n",
    "        years_group = pd.DataFrame(required_city_df.groupby(\"year\")[\"AQI\"].mean()).reset_index() # 4\n",
    "        mpl.style.use(\"Solarize_Light2\")  # 5\n",
    "        plt.figure(figsize=(20,5), dpi=300) # 5\n",
    "        sns.set(font_scale=2) # 5\n",
    "        sns.barplot(x=\"year\", y=\"AQI\", hue=\"year\", data=years_group, palette=\"coolwarm\").set(title=f\"yearly AQI of {required_city}\", xlabel=\"\", ylabel=\"AQI\") # 5\n",
    "        plt.legend().remove()  \n",
    "\n",
    "    elif year_or_month == 1: # monthly report\n",
    "\n",
    "        required_city_mask = station_merged_df[\"City\"] == required_city # 3\n",
    "        required_city_df = station_merged_df[required_city_mask] # 3\n",
    "        months_group = pd.DataFrame(required_city_df.groupby(\"month\")[\"AQI\"].mean()).reset_index() # 4\n",
    "        mpl.style.use(\"Solarize_Light2\") # 5\n",
    "        plt.figure(figsize=(20,5), dpi=300) # 5\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.plot([\"JAN\", \"FEB\", \"MAR\", \"APR\", \"MAY\", \"JUN\", \"JUL\", \"AUG\", \"SEPT\", \"OCT\",\"NOV\", \"DEC\"], months_group[\"AQI\"], marker=\"o\", c=\"purple\") # 5\n",
    "        plt.title(f\"aqi trend of {required_city} months from 2016-2020\")\n",
    "        plt.ylabel(\"AQI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_list = station_merged_df.City.unique()\n",
    "print(\"The number of cities avaible are: \", len(city_list))\n",
    "print()\n",
    "print(city_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "required_city = \"Chennai\"\n",
    "year_or_month = 0 # 0 for yearly report 1 for monthly\n",
    "city_Aqi_from_2015_to_2020(required_city, year_or_month)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* the aqi index of chennai has been steadily decreasing throughout the years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "required_city = \"Chennai\"\n",
    "year_or_month = 1 # 0 for yeqrly report 1 for monthly\n",
    "city_Aqi_from_2015_to_2020(required_city, year_or_month)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* jan seems to be the most toxic month for chennai as well"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### correlation of toxic gasses with aqi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_table = station_day_df.select_dtypes(include=['number']).corr()\n",
    "top_toxic_gases = correlation_table['AQI'].drop('AQI').sort_values(ascending=False)\n",
    "top_toxic_gases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(20, 8), dpi=300)\n",
    "\n",
    "sorted_correlation = correlation_table['AQI'].drop('AQI').sort_values(ascending=False)\n",
    "\n",
    "# Convert to DataFrame\n",
    "top_toxic_gases = sorted_correlation.reset_index()\n",
    "top_toxic_gases.columns = [\"Feature\", \"Correlation\"]\n",
    "\n",
    "# Create barplot with hue as the y variable\n",
    "ax = sns.barplot(\n",
    "    data=top_toxic_gases, \n",
    "    x=\"Correlation\", \n",
    "    y=\"Feature\", \n",
    "    hue=\"Feature\",\n",
    "    dodge=False,\n",
    "    palette=\"coolwarm\",\n",
    "    legend=False\n",
    ")\n",
    "\n",
    "# Add annotations (text on bars)\n",
    "for index, row in enumerate(top_toxic_gases.itertuples()):\n",
    "    ax.text(row.Correlation, index, f\"{row.Correlation:.2f}\", \n",
    "            ha='left', va='center', fontsize=12, color='black', fontweight='bold')\n",
    "\n",
    "# Labels & Title\n",
    "plt.title(\"Correlation of Air Pollutants with AQI\", fontsize=16)\n",
    "plt.xlabel(\"Correlation with AQI\", fontsize=14)\n",
    "plt.ylabel(\"Toxic Gases\", fontsize=14)\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.6)  # Optional: Add grid for readability\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* so from the above barplot we see the best 3 gasses are as follows\n",
    "* PM10 > PM2.5 > NOx | 0.89 > 0.81 > 0.51 respectively\n",
    "* so i will be plotting their mean values throughout the months and years of avaible states and citites in india"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------------------------------------------------\n",
    "# 7. yearly/monthly state_toxic_gas_from_2015_2020 india function\n",
    "\n",
    "def state_toxic_gas_from_2015_2020(required_state, year_or_month): # 1\n",
    "    \n",
    "    '''\n",
    "        this function shows the trend of PM10, PM2.5, NOx in a state from 2015-2020\n",
    "\n",
    "        steps:\n",
    "\n",
    "        1. get the year_or_month and required_state from the user\n",
    "        2. make a yearly trend report if 0 or a monthly trend report if 1\n",
    "        3. extract the states rows from the df and remove all unwanted columns\n",
    "        4. now from this df get the years/months for labels and extract mean values of all three pollutants from 2016-2020 in state\n",
    "        5. make the multiple bar graph by setting up tick locations and putting values extracted above\n",
    "    '''\n",
    "\n",
    "    if year_or_month == 0: # 2\n",
    "\n",
    "        required_state_mask = station_merged_df[\"State\"] == required_state # 3 \n",
    "        required_state_df = station_merged_df[required_state_mask] # 3 \n",
    "        required_state_df = required_state_df[['PM2_5', 'PM10','NOx','AQI','year']] # 3\n",
    "\n",
    "        year_group = required_state_df.groupby(\"year\") # 4\n",
    "        years = list(str(year) for year in year_group.groups.keys()) # 4\n",
    "        state_PM10 = required_state_df.groupby(\"year\")['PM10'].mean() # 4\n",
    "        state_PM25 = required_state_df.groupby(\"year\")['PM2_5'].mean() # 4\n",
    "        state_NOx = required_state_df.groupby(\"year\")['NOx'].mean() # 4\n",
    "\n",
    "        mpl.style.use(\"Solarize_Light2\")  # 5 \n",
    "        plt.figure(figsize=(20,7), dpi=300) # 5 \n",
    "\n",
    "        pm10_ticks = range(1, len(years)+1) # 5 \n",
    "        pm25_ticks = [x+0.2 for x in pm10_ticks] # 5 \n",
    "        NOx_ticks = [x-0.2 for x in pm10_ticks] # 5 \n",
    "\n",
    "        plt.bar(pm10_ticks, state_PM10, width=0.2, label=\"PM10\") # 5 \n",
    "        plt.bar(pm25_ticks,state_PM25, width=0.2, label=\"PM2_5\") # 5 \n",
    "        plt.bar(NOx_ticks,state_NOx, width=0.2, label=\"NOx\") # 5 \n",
    "\n",
    "        plt.title(f'Pollution levels in {required_state}') # 5 \n",
    "        plt.ylabel('Pollutant levels (in thier respective units)', fontsize=20) # 5 \n",
    "        plt.xticks(pm10_ticks, years) # 5 \n",
    "        plt.legend() # 5 \n",
    "        plt.show() # 5 \n",
    "\n",
    "    elif year_or_month == 1: # 2\n",
    "        \n",
    "        required_state_mask = station_merged_df[\"State\"] == required_state # 3 \n",
    "        required_state_df = station_merged_df[required_state_mask] # 3 \n",
    "        required_state_df = required_state_df[['PM2_5', 'PM10','NOx','AQI','month']] # 3\n",
    "\n",
    "        state_PM10 = list(required_state_df.groupby(\"month\")['PM10'].mean()) # 4\n",
    "        state_PM25 = list(required_state_df.groupby(\"month\")['PM2_5'].mean()) # 4\n",
    "        state_NOx = list(required_state_df.groupby(\"month\")['NOx'].mean()) # 4\n",
    "\n",
    "        mpl.style.use(\"Solarize_Light2\")  # 5 \n",
    "        plt.figure(figsize=(20,7)) # 5 \n",
    "\n",
    "        pm10_ticks = range(1, len(state_PM10)+ 1) # 5 \n",
    "        pm25_ticks = [x+0.2 for x in pm10_ticks] # 5 \n",
    "        NOx_ticks = [x-0.2 for x in pm10_ticks] # 5 \n",
    "\n",
    "        plt.bar(pm10_ticks, state_PM10, width=0.2, label=\"PM10\") # 5 \n",
    "        plt.bar(pm25_ticks,state_PM25, width=0.2, label=\"PM2.5\") # 5 \n",
    "        plt.bar(NOx_ticks,state_NOx, width=0.2, label=\"NOx\") # 5 \n",
    "        plt.title(f'Pollution levels in {required_state}') # 5 \n",
    "        plt.ylabel('Pollutant levels (in thier respective units)', fontsize=20) # 5 \n",
    "\n",
    "        if len(pm10_ticks) == 12:\n",
    "            plt.xticks(pm10_ticks, [\"JAN\", \"FEB\", \"MAR\", \"APR\", \"MAY\", \"JUN\", \"JUL\", \"AUG\", \"SEPT\", \"OCT\", \"NOV\", \"DEC\"]) # 5 \n",
    "        elif len(pm10_ticks) != 12:\n",
    "            plt.xticks(pm10_ticks, [\"JAN\", \"FEB\", \"MAR\", \"APR\", \"MAY\", \"JUN\", \"JUL\", \"SEPT\", \"OCT\", \"NOV\", \"DEC\"])\n",
    "            \n",
    "        plt.legend() # 5 \n",
    "        plt.show() # 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_list = station_merged_df.State.unique()\n",
    "print(\"The number of states avaible are: \", len(state_list))\n",
    "print()\n",
    "print(state_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "required_state = 'Andhra Pradesh'\n",
    "year_or_month = 0\n",
    "state_toxic_gas_from_2015_2020(required_state, year_or_month)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* if you see the yearly aqi graph of andra pradesh 2017 had the highest aqi\n",
    "* in our yearly toxic gass graph of andra pradesh 2017 has the highest values of these gasses\n",
    "* this proves the high corellation of these 3 gasses with aqi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "required_state = 'Andhra Pradesh'\n",
    "year_or_month = 1\n",
    "state_toxic_gas_from_2015_2020(required_state, year_or_month)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* the similar pattern can be seen in the monthly aqi graph of andrapradesh jan and dec peaked with the graph almost forming a u shape\n",
    "* the monthly toxic gass graph of andra pradesh shows the same u shape with the aqi peaks in jan and dec \n",
    "* this proves the high correlation between these 3 gasses and aqi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------------------------------------------------\n",
    "# 7. yearly/monthly city_toxic_gas_from_2015_2020 india function\n",
    "\n",
    "def city_toxic_gas_from_2015_2020(required_city, year_or_month): # 1\n",
    "    \n",
    "    '''\n",
    "        this function shows the trend of PM10, PM2.5, NOx in a city from 2015-2020\n",
    "\n",
    "        steps:\n",
    "\n",
    "        1. get the year_or_month and required_state from the user\n",
    "        2. make a yearly trend report if 0 or a monthly trend report if 1\n",
    "        3. extract the cities rows from the df and remove all unwanted columns\n",
    "        4. now from this df get the years/months for labels and extract mean values of all three pollutants from 2016-2020 in city\n",
    "        5. make the multiple bar graph by setting up tick locations and putting values extracted above\n",
    "    '''\n",
    "\n",
    "    if year_or_month == 0: # 2\n",
    "\n",
    "        required_city_mask = station_merged_df[\"City\"] == required_city # 3 \n",
    "        required_city_df = station_merged_df[required_city_mask] # 3 \n",
    "        required_city_df = required_city_df[['PM2_5', 'PM10','NOx','AQI','year']] # 3\n",
    "\n",
    "        year_group = required_city_df.groupby(\"year\") # 4\n",
    "        years = list(str(year) for year in year_group.groups.keys()) # 4\n",
    "        city_PM10 = required_city_df.groupby(\"year\")['PM10'].mean() # 4\n",
    "        city_PM25 = required_city_df.groupby(\"year\")['PM2_5'].mean() # 4\n",
    "        city_NOx = required_city_df.groupby(\"year\")['NOx'].mean() # 4\n",
    "\n",
    "        mpl.style.use(\"Solarize_Light2\")  # 5 \n",
    "        plt.figure(figsize=(20,7), dpi=300) # 5 \n",
    "\n",
    "        pm10_ticks = range(1, len(years)+1) # 5 \n",
    "        pm25_ticks = [x+0.2 for x in pm10_ticks] # 5 \n",
    "        NOx_ticks = [x-0.2 for x in pm10_ticks] # 5 \n",
    "\n",
    "        plt.bar(pm10_ticks, city_PM10, width=0.2, label=\"PM10\") # 5 \n",
    "        plt.bar(pm25_ticks,city_PM25, width=0.2, label=\"PM2_5\") # 5 \n",
    "        plt.bar(NOx_ticks,city_NOx, width=0.2, label=\"NOx\") # 5 \n",
    "\n",
    "        plt.title(f'Pollution levels in {required_city}') # 5 \n",
    "        plt.ylabel('Pollutant levels (in thier respective units)', fontsize=20) # 5 \n",
    "        plt.xticks(pm10_ticks, years) # 5 \n",
    "        plt.legend() # 5 \n",
    "        plt.show() # 5 \n",
    "\n",
    "    elif year_or_month == 1: # 2\n",
    "        \n",
    "        required_city_mask = station_merged_df[\"City\"] == required_city # 3 \n",
    "        required_city_df = station_merged_df[required_city_mask] # 3 \n",
    "        required_city_df = required_city_df[['PM2_5', 'PM10','NOx','AQI','month']] # 3\n",
    "\n",
    "        city_PM10 = list(required_city_df.groupby(\"month\")['PM10'].mean()) # 4\n",
    "        city_PM25 = list(required_city_df.groupby(\"month\")['PM2_5'].mean()) # 4\n",
    "        city_NOx = list(required_city_df.groupby(\"month\")['NOx'].mean()) # 4\n",
    "\n",
    "        mpl.style.use(\"Solarize_Light2\")  # 5 \n",
    "        plt.figure(figsize=(20,7)) # 5 \n",
    "\n",
    "        pm10_ticks = range(1, len(city_PM10)+ 1) # 5 \n",
    "        pm25_ticks = [x+0.2 for x in pm10_ticks] # 5 \n",
    "        NOx_ticks = [x-0.2 for x in pm10_ticks] # 5 \n",
    "\n",
    "        plt.bar(pm10_ticks, city_PM10, width=0.2, label=\"PM10\") # 5 \n",
    "        plt.bar(pm25_ticks,city_PM25, width=0.2, label=\"PM2_5\") # 5 \n",
    "        plt.bar(NOx_ticks,city_NOx, width=0.2, label=\"NOx\") # 5 \n",
    "        plt.title(f'Pollution levels in {required_city}') # 5 \n",
    "        plt.ylabel('Pollutant levels (in thier respective units)', fontsize=20) # 5 \n",
    "\n",
    "        if len(pm10_ticks) == 12:\n",
    "            plt.xticks(pm10_ticks, [\"JAN\", \"FEB\", \"MAR\", \"APR\", \"MAY\", \"JUN\", \"JUL\", \"AUG\", \"SEPT\", \"OCT\", \"NOV\", \"DEC\"]) # 5 \n",
    "        elif len(pm10_ticks) != 12:\n",
    "            plt.xticks(pm10_ticks, [\"JAN\", \"FEB\", \"MAR\", \"APR\", \"MAY\", \"JUN\", \"JUL\", \"SEPT\", \"OCT\", \"NOV\", \"DEC\"])\n",
    "            \n",
    "        plt.legend() # 5 \n",
    "        plt.show() # 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_list = station_merged_df.City.unique()\n",
    "print(\"The number of cities avaible are: \", len(city_list))\n",
    "print()\n",
    "print(city_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "required_city = \"Chennai\"\n",
    "year_or_month = 0\n",
    "city_toxic_gas_from_2015_2020(required_city, year_or_month)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* if you check the aqi of chennai on the code above it shows a similar trend of it decreasing steadily over time\n",
    "* this proves that the correlation of gasses with aqi is strong even on a city level\n",
    "* lets check the monthly toxic gas trend in channai "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "required_city = \"Chennai\"\n",
    "year_or_month = 1\n",
    "city_toxic_gas_from_2015_2020(required_city, year_or_month)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* also a similar trend to monthly aqi of channai "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### by this analysis we can conclude that aqi of a city/ state highly depends on PM10, PM2.5, NOx levels in the air more insights are stated above\n",
    "\n",
    "### finnaly lets conclude this notebook by showing the sates of these of states/cities on an indian map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------------------------------------------------------------------------------------------------\n",
    "# 9. aqi of states on an india map\n",
    "\n",
    "# states_geo_data =  gpd.read_file(\"india_state.geojson\")\n",
    "# sates_geo_data.head(2)\n",
    "\n",
    "import json\n",
    "import geopandas as gpd\n",
    "\n",
    "# Load GeoJSON manually\n",
    "with open(\"india_state.geojson\", \"r\") as f:\n",
    "    geojson_data = json.load(f)\n",
    "\n",
    "# Convert to GeoDataFrame\n",
    "states_geo_data = gpd.GeoDataFrame.from_features(geojson_data[\"features\"])\n",
    "\n",
    "# ðŸ”¹ Set CRS to WGS84 (EPSG:4326) required by Folium\n",
    "states_geo_data.set_crs(epsg=4326, inplace=True)\n",
    "\n",
    "# ðŸ”¹ Convert GeoDataFrame to JSON for Folium\n",
    "states_geo_data = states_geo_data.to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "india_state_map = folium.Map(location=[22.5937, 78.9629], zoom_start=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_geo_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "india_state_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folium.Choropleth(\n",
    "    geo_data=states_geo_data,\n",
    "    data=station_merged_df,  \n",
    "    columns=['State', 'AQI'],\n",
    "    key_on='feature.properties.NAME_1',\n",
    "    fill_color='YlOrRd', \n",
    "    fill_opacity=0.7, \n",
    "    line_opacity=0.7,\n",
    "    legend_name='AIR QUALITY INDEX OF STATES IN INDIA'\n",
    ").add_to(india_state_map)\n",
    "\n",
    "india_state_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folium.Choropleth(\n",
    "    geo_data=states_geo_data,\n",
    "    data=station_merged_df,  \n",
    "    columns=['State', 'AQI'],\n",
    "    key_on='feature.properties.NAME_1',\n",
    "    fill_color='YlOrRd', \n",
    "    fill_opacity=0.7, \n",
    "    line_opacity=0.7,\n",
    "    legend_name='AIR QUALITY INDEX OF STATES IN INDIA'\n",
    ").add_to(india_state_map)\n",
    "\n",
    "india_state_map"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* From the map above the norhtern india seems to be the most poulated \n",
    "* while jammu and kashmir and eastern southern and central india seem to be doing good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------------------------------------------------\n",
    "# 9. aqi of cities on an india map\n",
    "\n",
    "# Load GeoJSON manually\n",
    "with open(\"/Users/pulkitsoni/Air_Quality_Analysis_and_Prediction_for_Indian_Cities_and_States/india_district.geojson\", \"r\") as f:\n",
    "    geojson_data = json.load(f)\n",
    "\n",
    "# Convert to GeoDataFrame\n",
    "cities_geo_data = gpd.GeoDataFrame.from_features(geojson_data[\"features\"])\n",
    "\n",
    "# ðŸ”¹ Set CRS to WGS84 (EPSG:4326), required for mapping\n",
    "cities_geo_data.set_crs(epsg=4326, inplace=True)\n",
    "\n",
    "# Check output (should be the same as `gpd.read_file()`)\n",
    "print(cities_geo_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "india_city_map = folium.Map(location=[22.5937, 78.9629], zoom_start=5)\n",
    "\n",
    "folium.Choropleth(\n",
    "    geo_data=cities_geo_data,  # Ensure this file contains city boundaries\n",
    "    data=station_merged_df,\n",
    "    columns=['City', 'AQI'],\n",
    "    key_on='feature.properties.NAME_1',  # Ensure city names match in both datasets\n",
    "    fill_color='YlOrRd',\n",
    "    fill_opacity=0.7,\n",
    "    line_opacity=0.7,\n",
    "    legend_name='AIR QUALITY INDEX OF CITIES IN INDIA'\n",
    ").add_to(india_city_map)\n",
    "\n",
    "india_city_map\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Online avaible json files only have data on chandighar and delhi which is why these are the only coloured ones\n",
    "* Other cities have HIGH aqi but it wont show on this map as there data isnt avaible in the json file\n",
    "\n",
    "### Take this map with a grain of salt the information above is misleading as some cities with higher aqi are lighter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is it for preliminary and exploratory analysis of station merged df\n",
    "# ______________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 2 --> MODAL ANALYSIS \n",
    "<b> This is the second part of AQI_analysis_of_indian_states_and_cities PROJECT which proccess the data from station_merged csv and  trains the models. </b>\n",
    "\n",
    "# objective \n",
    "\n",
    "<b> Use historical data of air quality readings from india to predict air quality index of a given region. </b>\n",
    "\n",
    "# what does this notebook do?\n",
    "\n",
    "<b> \n",
    "\n",
    "* https://www.diva-portal.org/smash/get/diva2:1681590/FULLTEXT02 in this reasearch paper the creators have only dumped the data into their machine learning models but did not explore the data itself. \n",
    "\n",
    "* The models they used achived (Ridge regression {mea: 27.907 rmse: 36.791, r2: 0.8089}) score so we will try to build a better model in this ipynb\n",
    "\n",
    "</b>\n",
    "<b> \n",
    "\n",
    "* The (xgb_model mea = 21.22, rmse = 37.011, r2 = 0.91) is made in this csv \n",
    "* This model will be used by PART 3 (3_main_AQI_predictor.pyw) to make predictions and show aqi \n",
    "\n",
    "# ________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import math\n",
    "import joblib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ________________________________________________________________________________\n",
    "# 2. reading the station_merged_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_merged_data = pd.read_csv(\"/Users/pulkitsoni/Air_Quality_Analysis_and_Prediction_for_Indian_Cities_and_States/station_merged.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ________________________________________________________________________________\n",
    "# 3. preliminary and exploratory data analysis of station_merged_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "checking if the station_merged_df \n",
    "0. things changed in the station_merged_csv\n",
    "1. general shape \n",
    "2. some values\n",
    "3. columns\n",
    "4. data distribution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. things changed in the station_merged_csv\n",
    "\n",
    "* all of this stuff was changed in exploratory_analysis_of_aqi_in_india_1.ipynb\n",
    "\n",
    "* dropped all columns where aqi reading was none\n",
    "* numerical missing values were replaced with means of the respective columns \n",
    "* dropped the date column and added day, month, year columns instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. general shape\n",
    "station_merged_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. some values\n",
    "station_merged_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. columns\n",
    "station_merged_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### in station merged df\n",
    "\n",
    "| #  | Column     | Non-Null Count  | Dtype   |    meaning   |\n",
    "|:-: | :------:     | :--------------:  | :-----:   | :---:  |\n",
    "| 0  | StationId  | 108035 non-null | object  |    id of the station   |\n",
    "| 1  | City       | 108035 non-null | object  |    city in which the station is located   |\n",
    "| 2  | State      | 108035 non-null | object  |    state in which the station is located   |\n",
    "| _  | Date       | 108035 non-null | object  |    date of the readings (now removed)   |\n",
    "| 3  | PM2.5      | 86410 non-null  | float64 |    amount of said checmical in the air   |\n",
    "| 4  | PM10       | 65329 non-null  | float64 |    amount of said checmical in the air   |\n",
    "| 5  | NO         | 90929 non-null  | float64 |    amount of said checmical in the air   |\n",
    "| 6  | NO2        | 91488 non-null  | float64 |    amount of said checmical in the air   |\n",
    "| 7  | NOx        | 92535 non-null  | float64 |    amount of said checmical in the air   |\n",
    "| 8  | NH3        | 59930 non-null  | float64 |    amount of said checmical in the air   |\n",
    "| 9  | CO         | 95037 non-null  | float64 |    amount of said checmical in the air   |\n",
    "| 10 | SO2        | 82831 non-null  | float64 |    amount of said checmical in the air   |\n",
    "| 11 | O3         | 82467 non-null  | float64 |    amount of said checmical in the air   |\n",
    "| 12 | Benzene    | 76580 non-null  | float64 |    amount of said checmical in the air   |\n",
    "| 13 | Toluene    | 69333 non-null  | float64 |    amount of said checmical in the air   |\n",
    "| 14 | Xylene     | 22898 non-null  | float64 |    amount of said checmical in the air   |\n",
    "| 15 | AQI        | 87025 non-null  | float64 |    the air quality that day   |\n",
    "| 16 | AQI_Bucket | 87025 non-null  | object  |    AQI catagory ranges from good to severe   |\n",
    "| 17 | year       | 87025 non-null  | float64 |    year of the reading   |\n",
    "| 18 | month      | 87025 non-null  | float64 |    month of the reading   |\n",
    "| 18 | date       | 87025 non-null  | float64 |    day of the reading   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. columns\n",
    "station_merged_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_merged_data = station_merged_data.drop(columns=[\"StationId\", \"City\", \"State\", \"year\", \"month\", \"day\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_merged_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* we really dont need the StationId, City, State, year, month, date columns so we will remove them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. data distribution\n",
    "station_merged_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### most of the preliminary alanlysis has been done in the other ipynb this ipynb concentrates on the data distribution and model evaluation\n",
    "# ___________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory analysis:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heavy exploratory analysis of the dataset has already been done in the exploratory_analysis_of_aqi_in_india_1.ipynb i would highly prefer you read it first then come and check this csv out \n",
    "\n",
    "* lets check the distribution of values in each of the columns  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution of aqi from 2015-2020\n",
    "plt.figure(figsize=(12, 6), dpi=300)  # High resolution with 300 DPI\n",
    "sns.displot(station_merged_data, x=\"AQI\", color=\"red\")\n",
    "plt.title(\"Distribution of AQI\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* the avarage aqi across 87000 readings from station all over india has spike in the range of 100-200 and another spike in the range of 300-400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution of values of toxic gasses in the air\n",
    "station_merged_data[[i for i in station_merged_data.columns if i not in [\"AQI_Bucket\",\"AQI\", \"year\", \"month\", \"day\"]]].hist(bins=30, figsize=(20, 12), color=\"green\", legend=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* benzene, xylene and etc are usually 0 so they are not a problem\n",
    "* but PM10, PM2.5, NO2, NOx, NH3 have high levels of somewhere bettween 100-200 across 87000 readings from station all over india "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lets check the coorelation of aqi with toxic gases using a heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_table = station_merged_data.select_dtypes(include=['number']).corr()\n",
    "top_toxic_gases = correlation_table['AQI'].drop('AQI').sort_values(ascending=False)\n",
    "top_toxic_gases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 8), dpi=300)\n",
    "\n",
    "# Take the top 3 toxic gases\n",
    "top_toxic_gases = top_toxic_gases.reset_index()\n",
    "top_toxic_gases.columns = [\"Feature\", \"Correlation\"]\n",
    "\n",
    "# Create barplot\n",
    "sns.barplot(data=top_toxic_gases, x=\"Correlation\", y=\"Feature\", hue=\"Feature\", palette=\"coolwarm\", dodge=False, legend=False)\n",
    "\n",
    "# Labels & Title\n",
    "plt.title(\"Correlation of Toxic Gases with AQI\")\n",
    "plt.xlabel(\"Correlation with AQI\")\n",
    "plt.ylabel(\"Toxic Gases\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* from the above table the top contenders for growth of aqi are PM2.5, PM19, NO, NO2, NOx, CO, SO2, NH3, TOULENE in this order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### this does it for exploratory analysis of aqi station readings across india now its time to test and train the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ________________________________________________________________________________________________________\n",
    "# 4. data preproccessing and spiliting our dataset into target_y and features_x dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we wont need the aqi bucket column as we will use regression algorithms\n",
    "station_merged_data = station_merged_data.drop(columns=\"AQI_Bucket\")\n",
    "target_y = station_merged_data[\"AQI\"]\n",
    "features_x = pd.DataFrame(station_merged_data.drop(columns=\"AQI\"))\n",
    "\n",
    "target_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_x.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### why we are scaling our dataframe\n",
    "\n",
    "* the data in features_x is on diffrent scale as in PM10 is in between 0.03 - 976.77 on the other hand CO is in range of 0.01 - 186.08\n",
    "* if we feed this data to our models straight up it will be biased towards higher values in some models\n",
    "* therefore we will standardize our value between -3 - 3 so our models generalize better to future values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "features_x = pd.DataFrame(scaler.fit_transform(features_x),columns=features_x.columns)\n",
    "features_x.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* notice how the mean and standard deviation of features_x has changed the mean is now 0 and standard deviation now 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_x.hist(bins=30, figsize=(20,10), color=\"purple\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* notice how the x axis for most of the values has changed and everything has came down to a similar scale\n",
    "* notice distribution of our data it is \"right skewed\" \n",
    "* the min and max values of our columns range from -ve to +ve so we will use yeo transform to normalise some of our columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using yeo transform to transform features_x \n",
    "\n",
    "pt = PowerTransformer()\n",
    "\n",
    "features_x_transformed = pt.fit_transform(features_x)\n",
    "features_x_transformed = pd.DataFrame(features_x_transformed, columns=features_x.columns)\n",
    "for col in features_x_transformed.columns:\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1,2, figsize=(20,3))\n",
    "    sns.histplot(features_x[col],kde=True,color=\"red\", ax=ax1)\n",
    "    ax1.set_title(str(col))\n",
    "    sns.histplot(features_x_transformed[col],color=\"yellow\",kde=True,ax=ax2)\n",
    "    ax2.set_title(str(col+\"_transformed\"))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_x = features_x_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* the yeo transformation has done its wonders and most of the columns are now normalised making our data a good fit for XGBOOST reggressor multiple linear regression and svr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _______________________________________________________________________________________________\n",
    "# 5. using train test split and fitting our data into ml models of choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(features_x, target_y, random_state=0)\n",
    "\n",
    "print(f\"training data --> X_train: {X_train.shape}, Y_train: {Y_train.shape}\")\n",
    "print(f\"testing data --> X_test: {X_test.shape}, Y_test: {Y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, BayesianRidge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting our data into a multiple linear regression model and storing the scores in results \n",
    "\n",
    "mlr_model = LinearRegression().fit(X_train, Y_train)\n",
    "xgb_model = XGBRegressor().fit(X_train, Y_train)\n",
    "svr_model = SVR(kernel='rbf', C=1.0, epsilon=0.1).fit(X_train, Y_train)\n",
    "ridge_model = Ridge(alpha=1.0).fit(X_train, Y_train)\n",
    "lasso_model = Lasso(alpha=1.0).fit(X_train, Y_train)\n",
    "elastic_model = ElasticNet(alpha=1.0, l1_ratio=0.5).fit(X_train, Y_train)\n",
    "dt_model = DecisionTreeRegressor().fit(X_train, Y_train)\n",
    "rf_model = RandomForestRegressor(n_estimators=100).fit(X_train, Y_train)\n",
    "gbm_model = GradientBoostingRegressor(n_estimators=100).fit(X_train, Y_train)\n",
    "lgbm_model = LGBMRegressor().fit(X_train, Y_train)\n",
    "catboost_model = CatBoostRegressor(verbose=0).fit(X_train, Y_train)\n",
    "knn_model = KNeighborsRegressor(n_neighbors=5).fit(X_train, Y_train)\n",
    "bayesian_model = BayesianRidge().fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlr_model_prediction = mlr_model.predict(X_test)\n",
    "xgb_model_prediction = xgb_model.predict(X_test)\n",
    "svr_model_prediction = svr_model.predict(X_test)\n",
    "ridge_model_prediction = ridge_model.predict(X_test)\n",
    "lasso_model_prediction = lasso_model.predict(X_test)\n",
    "elastic_model_prediction = elastic_model.predict(X_test)\n",
    "dt_model_prediction = dt_model.predict(X_test)\n",
    "rf_model_prediction = rf_model.predict(X_test)\n",
    "gbm_model_prediction = gbm_model.predict(X_test)\n",
    "lgbm_model_prediction = lgbm_model.predict(X_test)\n",
    "catboost_model_prediction = catboost_model.predict(X_test)\n",
    "knn_model_prediction = knn_model.predict(X_test)\n",
    "bayesian_model_prediction = bayesian_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ________________________________________________________________________\n",
    "# 6. evaluating our models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mea_scores = []\n",
    "cross_val_scores = []\n",
    "\n",
    "# Training and Testing Scores\n",
    "training_scores = [mlr_model.score(X_train, Y_train), xgb_model.score(X_train, Y_train), \n",
    "                   svr_model.score(X_train, Y_train), ridge_model.score(X_train, Y_train), \n",
    "                   lasso_model.score(X_train, Y_train), elastic_model.score(X_train, Y_train),\n",
    "                   dt_model.score(X_train, Y_train), rf_model.score(X_train, Y_train), \n",
    "                   gbm_model.score(X_train, Y_train), lgbm_model.score(X_train, Y_train),\n",
    "                   catboost_model.score(X_train, Y_train), knn_model.score(X_train, Y_train),\n",
    "                   bayesian_model.score(X_train, Y_train)]\n",
    "\n",
    "testing_scores = [metrics.r2_score(Y_test, mlr_model_prediction), metrics.r2_score(Y_test, xgb_model_prediction), \n",
    "                  metrics.r2_score(Y_test, svr_model_prediction), metrics.r2_score(Y_test, ridge_model_prediction),\n",
    "                  metrics.r2_score(Y_test, lasso_model_prediction), metrics.r2_score(Y_test, elastic_model_prediction),\n",
    "                  metrics.r2_score(Y_test, dt_model_prediction), metrics.r2_score(Y_test, rf_model_prediction),\n",
    "                  metrics.r2_score(Y_test, gbm_model_prediction), metrics.r2_score(Y_test, lgbm_model_prediction),\n",
    "                  metrics.r2_score(Y_test, catboost_model_prediction), metrics.r2_score(Y_test, knn_model_prediction),\n",
    "                  metrics.r2_score(Y_test, bayesian_model_prediction)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_scores = [cross_val_score(mlr_model, features_x, target_y, cv=3, scoring='neg_mean_squared_error').mean(),\n",
    "                    cross_val_score(xgb_model, features_x, target_y, cv=3, scoring='neg_mean_squared_error').mean(),\n",
    "                    cross_val_score(svr_model, features_x, target_y, cv=3, scoring='neg_mean_squared_error').mean(),\n",
    "                    cross_val_score(ridge_model, features_x, target_y, cv=3, scoring='neg_mean_squared_error').mean(),\n",
    "                    cross_val_score(lasso_model, features_x, target_y, cv=3, scoring='neg_mean_squared_error').mean(),\n",
    "                    cross_val_score(elastic_model, features_x, target_y, cv=3, scoring='neg_mean_squared_error').mean(),\n",
    "                    cross_val_score(dt_model, features_x, target_y, cv=3, scoring='neg_mean_squared_error').mean(),\n",
    "                    cross_val_score(rf_model, features_x, target_y, cv=3, scoring='neg_mean_squared_error').mean(),\n",
    "                    cross_val_score(gbm_model, features_x, target_y, cv=3, scoring='neg_mean_squared_error').mean(),\n",
    "                    cross_val_score(lgbm_model, features_x, target_y, cv=3, scoring='neg_mean_squared_error').mean(),\n",
    "                    cross_val_score(catboost_model, features_x, target_y, cv=3, scoring='neg_mean_squared_error').mean(),\n",
    "                    cross_val_score(knn_model, features_x, target_y, cv=3, scoring='neg_mean_squared_error').mean(),\n",
    "                    cross_val_score(bayesian_model, features_x, target_y, cv=3, scoring='neg_mean_squared_error').mean()]\n",
    "\n",
    "mea_scores = [mean_absolute_error(mlr_model_prediction, Y_test), mean_absolute_error(xgb_model_prediction, Y_test), \n",
    "              mean_absolute_error(svr_model_prediction, Y_test), mean_absolute_error(ridge_model_prediction, Y_test), \n",
    "              mean_absolute_error(lasso_model_prediction, Y_test), mean_absolute_error(elastic_model_prediction, Y_test), \n",
    "              mean_absolute_error(dt_model_prediction, Y_test), mean_absolute_error(rf_model_prediction, Y_test), \n",
    "              mean_absolute_error(gbm_model_prediction, Y_test), mean_absolute_error(lgbm_model_prediction, Y_test), \n",
    "              mean_absolute_error(catboost_model_prediction, Y_test), mean_absolute_error(knn_model_prediction, Y_test), \n",
    "              mean_absolute_error(bayesian_model_prediction, Y_test)]\n",
    "\n",
    "print(training_scores)\n",
    "print(testing_scores)\n",
    "print(cross_val_scores)\n",
    "print(mea_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* in the code below cross_val_scores has the mean of all 3 cross_val_score of the model\n",
    "* mea_scores is now at the correct place\n",
    "* testing score are now at the correct place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlr_cv_scores = cross_val_score(mlr_model, X_train, Y_train, cv=5)\n",
    "xgb_cv_scores = cross_val_score(xgb_model, X_train, Y_train, cv=5)\n",
    "svr_cv_scores = cross_val_score(svr_model, X_train, Y_train, cv=5)\n",
    "ridge_cv_scores = cross_val_score(ridge_model, X_train, Y_train, cv=5)\n",
    "lasso_cv_scores = cross_val_score(lasso_model, X_train, Y_train, cv=5)\n",
    "elastic_cv_scores = cross_val_score(elastic_model, X_train, Y_train, cv=5)\n",
    "dt_cv_scores = cross_val_score(dt_model, X_train, Y_train, cv=5)\n",
    "rf_cv_scores = cross_val_score(rf_model, X_train, Y_train, cv=5)\n",
    "gbm_cv_scores = cross_val_score(gbm_model, X_train, Y_train, cv=5)\n",
    "lgbm_cv_scores = cross_val_score(lgbm_model, X_train, Y_train, cv=5)\n",
    "catboost_cv_scores = cross_val_score(catboost_model, X_train, Y_train, cv=5)\n",
    "knn_cv_scores = cross_val_score(knn_model, X_train, Y_train, cv=5)\n",
    "bayesian_cv_scores = cross_val_score(bayesian_model, X_train, Y_train, cv=5)\n",
    "\n",
    "# Now calculate the average of each model's cross-validation scores\n",
    "cross_val_scores = [sum(mlr_cv_scores)/len(mlr_cv_scores), \n",
    "                    sum(xgb_cv_scores)/len(xgb_cv_scores),\n",
    "                    sum(svr_cv_scores)/len(svr_cv_scores),\n",
    "                    sum(ridge_cv_scores)/len(ridge_cv_scores),\n",
    "                    sum(lasso_cv_scores)/len(lasso_cv_scores),\n",
    "                    sum(elastic_cv_scores)/len(elastic_cv_scores),\n",
    "                    sum(dt_cv_scores)/len(dt_cv_scores),\n",
    "                    sum(rf_cv_scores)/len(rf_cv_scores),\n",
    "                    sum(gbm_cv_scores)/len(gbm_cv_scores),\n",
    "                    sum(lgbm_cv_scores)/len(lgbm_cv_scores),\n",
    "                    sum(catboost_cv_scores)/len(catboost_cv_scores),\n",
    "                    sum(knn_cv_scores)/len(knn_cv_scores),\n",
    "                    sum(bayesian_cv_scores)/len(bayesian_cv_scores)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\"mlr_model\", \"xgb_model\",\"svr_model\", \"ridge_model\", \"lasso_model\", \"elastic_model\", \"dt_model\", \"rf_model\", \"gbm_model\", \"lgbm_model\", \"catboost_model\", \"knn_model\", \"bayesian_model\"]\n",
    "final_scores_df = pd.DataFrame({\"model_names\":model_names ,\"training_scores\": training_scores,\"testing_scores\": testing_scores, \"mea_scores\": mea_scores, \"cross_val_scores\": cross_val_scores}).round(2)\n",
    "final_scores_df = final_scores_df.sort_values(by=\"cross_val_scores\")\n",
    "final_scores_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* âœ… 1. Random Forest (rf_model)\n",
    "* Training Score: 0.99\n",
    "* Testing Score: 0.92\n",
    "* MAE: 20.82 (Lowest)\n",
    "* Cross-Val Score: 0.92\n",
    "\n",
    "* âœ… 2. LightGBM (lgbm_model)\n",
    "* Training Score: 0.94\n",
    "* Testing Score: 0.92\n",
    "* MAE: 21.45\n",
    "* Cross-Val Score: 0.92\n",
    "\n",
    "* âœ… 3. CatBoost (catboost_model)\n",
    "* Training Score: 0.95\n",
    "* Testing Score: 0.92\n",
    "* MAE: 20.91\n",
    "* Cross-Val Score: 0.92\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Random Forest is the best model overall (lowest MAE and high scores).\n",
    "* LightGBM and CatBoost are very close competitors.\n",
    "* Decision Tree (dt_model) is overfitting (Training Score: 1.00, but Testing Score: 0.84).\n",
    "* Elastic Net, MLR, Ridge, and Bayesian models perform the worst across all metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.style.use(\"Solarize_Light2\")\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2,figsize=(20, 8))\n",
    "\n",
    "ax1.plot(model_names,final_scores_df[\"training_scores\"],color=\"orange\", marker=\"o\")\n",
    "ax1.set_title(\"training_scores\")\n",
    "\n",
    "ax2.plot(model_names,final_scores_df[\"testing_scores\"],color=\"green\", marker=\"o\")\n",
    "ax2.set_title(\"testing_scores\")\n",
    "\n",
    "ax3.plot(model_names,final_scores_df[\"mea_scores\"],color=\"Blue\", marker=\"o\")\n",
    "ax3.set_title(\"mea_scores\")\n",
    "\n",
    "ax4.plot(model_names,final_scores_df[\"cross_val_scores\"],color=\"purple\", marker=\"o\")\n",
    "ax4.set_title(\"cross_val_scores\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model_mse = metrics.mean_squared_error(rf_model_prediction, Y_test)\n",
    "rf_model_mea = metrics.mean_absolute_error(rf_model_prediction, Y_test)\n",
    "rf_model_rmse = math.sqrt(rf_model_mse)\n",
    "\n",
    "print(\"The mean absolute error of the model is : \",rf_model_mea.round(2))\n",
    "print(\"The mean squared error of the model is : \",rf_model_mse.round(2))\n",
    "print(\"The mean root mean squared error of the model is : \",rf_model_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the xgbRegressor model dosent need the data to be scaled and yeo transformed so lets try find out how good that model is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = station_merged_data[\"AQI\"]\n",
    "X = station_merged_data.drop(columns=[\"AQI\"])\n",
    "\n",
    "# these are spllited versions of nomral data which is not scaled not yeo transformed\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state=0)\n",
    "\n",
    "normal_rf_model = RandomForestRegressor().fit(X_train, Y_train)\n",
    "normal_rf_model_prediction = normal_rf_model.predict(X_test)\n",
    "\n",
    "print(\" --- stats of normal_rf_model --- \")\n",
    "print(\"Training score: \", round(normal_rf_model.score(X_train, Y_train), 2))\n",
    "print(\"Testing score: \", round(metrics.r2_score(normal_rf_model_prediction, Y_test), 2))\n",
    "print(\"Mean absolute error: \", round(metrics.mean_absolute_error(normal_rf_model_prediction, Y_test), 2))\n",
    "print(\"Mean squared error: \", round(metrics.mean_squared_error(normal_rf_model_prediction, Y_test), 2))\n",
    "print(\"Root mean squared error: \", round(math.sqrt(metrics.mean_squared_error(normal_rf_model_prediction, Y_test)), 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ______________________________________________________________________________________________\n",
    "# 7. conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### from the above tables and plots we can conclude these things\n",
    "* RandomForestRegressor is the fastest and the best model with a cross_val_score of 0.92 and testing score of 0.92\n",
    "* the best mea_score i could get was 21.45(LGBM) which is not bad as aqi ranges from 0 - 400 but it can still be worked on\n",
    "* Elastic Net, MLR, Ridge, and Bayesian models perform the worst across all metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. has the main goal of the notebook been achived?\n",
    "\n",
    "* The best model in the research paper was (Ridge regression mea = 27.907, rmse = 36.791, r2 = 0.8089) on city_data csv\n",
    "* my best model in this notebook was (Randon_Forest mea = 20.73, rmse = 36.97, r2 = 0.91) on station_data csv\n",
    "\n",
    "* the city_data csv has only 30000 rows while station_data csv has 100000 which is way more data \n",
    "* my model has achived better mea and r2 scores on a bigger scale of data making it better than the ridge reggression model made in the research paper\n",
    "\n",
    "* but my is not the best and can still be upgraded "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _________________________________________________________________________________________________________\n",
    "# 9. saving the model into a binary file so we can use it in the GUI program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### since the xgbr model dosent need the data to be yeo transfoemed and scaled we will fit the normal data into it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model_file_path = \"/Users/pulkitsoni/Air_Quality_Analysis_and_Prediction_for_Indian_Cities_and_States/rf_model\"\n",
    "joblib.dump(normal_rf_model, rf_model_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the feature importances\n",
    "importances = normal_rf_model.feature_importances_\n",
    "\n",
    "# Get the feature names\n",
    "features = X.columns\n",
    "\n",
    "# Sort the features by importance\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Plot the feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(len(importances)), importances[indices], align=\"center\")\n",
    "plt.xticks(range(len(importances)), features[indices], rotation=90)\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Importance')\n",
    "plt.title('Feature Importance (RandomForestRegressor)')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
